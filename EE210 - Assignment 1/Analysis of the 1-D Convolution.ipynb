{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an [jupyter](http://jupyter.org) notebook.\n",
    "Lectures about Python, useful both for beginners and experts, can be found at http://scipy-lectures.github.io.\n",
    "\n",
    "Open the notebook by (1) copying this file into a directory, (2) in that directory typing \n",
    "jupyter-notebook\n",
    "and (3) selecting the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color= 'blue'>Analysis of the 1-D Convolution Operation </font>\n",
    "### <i>Practical Assignment 1, EE210 (Spring 2017) </i>\n",
    "___\n",
    "A notebook by ___Dhruv Ilesh Shah___ and ___Shashwat Shukla___  \n",
    "___\n",
    "__Required Packages:__ Python(2.7+), NumPy, math, cmath, SciPy.io _(Optional, to simplify IO operations)_\n",
    "___\n",
    "In this notebook, we present an analysis of the various algorithms for computing the one-dimensional convolution operation, along with their implementation and suitable improvisations. This includes the crude implementation, parallelised implementation, Cooley-Tukey algorithm (FFT), improvised Cooley-Tukey and a vectorised FFT-based implementation. We will look at the Python and C++ implementations on both CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "The convolution operation is a fundamental operation on two functions $f$ and $g$ which produces a third function, that is typically viewed as a modified version of one of the original functions, giving the integral of the pointwise multiplication of the two functions as a function of the amount that one of the original functions is translated. Convolution is similar to _cross-correlation_.\n",
    "\n",
    "$$\n",
    "(f*g)(t) \\triangleq \\int_{-\\infty}^{\\infty} f(\\tau) g(t - \\tau)\n",
    "$$\n",
    "Where $f, g : [0, \\infty) \\rightarrow \\mathbb{R}$.\n",
    "This can be extended to discrete-time signals as\n",
    "$$\n",
    "(f*g)[n] \\triangleq \\sum_{m = -\\infty}^{\\infty} f[m] g[n - m]\n",
    "$$\n",
    "\n",
    "For any physical _linear & time invariant (LTI)_ system $T$, it can be shown that the output of $T$ can be represented as $y(t) = x(t) * h(t)$, where $h(t)$ represents the response of $T$ to the unit impulse. This means that given an LTI system, the complete system can be represented solely by its impulse response $h(t)$. _(Similar definition for discrete-time signals.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "Given a single-channel audio input $(16kHz)$, and the 2-channel impulse response of the surroundings as heard by the pair of human ears, generate a stereo reconstruction of the input.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y_l (t) &= x (t) * h_l (t) \\\\\n",
    "y_r (t) &= x (t) * h_r (t)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "***\n",
    "In a typical scenario, the impulse response depends on the surroundings due to multiple reflections, interference and reverberation etc. The energy of the response, however, decays slowly and we have a finite duration response.\n",
    "![Impulse Response of a Hall](http://www.prosoundweb.com/images/uploads/RationalGuideFigure1June2015.jpg)\n",
    "<center>Courtesy: [ProSoundWeb](http://www.prosoundweb.com/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "As is clear from the definition, the convolution is an expensive operation and given the size of a typical sound clip $(10 sec \\rightarrow 1.6\\times10^5samples)$, the computation can be very intensive. But given the essentiality of the convolution as a fundamental property of an LTI system, and the immense number of applications, it cannot be ignored. Hence, we present an analysis of various ways to compute this convolution by employing certain other beautiful properties of this operation. We begin with algorithmic analysis for various methods to compute the convolution, and then go ahead to solve the actual problem with the best implementation. The topics covered are listed below:\n",
    "\n",
    "\n",
    "1. Naive Implementation\n",
    "2. Naive Implementation Parallelised\n",
    "3. Switching to the Frequency Domain  \n",
    "      a. Naive Fast Fourier Transform  \n",
    "      b. Cooley-Tukey Algorithm  \n",
    "      c. Cooley-Tukey Algorithm (Improvisations)\n",
    "\n",
    "Towards the end, we use an improvised version of the _Cooley-Tukey Algorithm_ to compute the linear convolution of the given signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> Naive Implementation </font>\n",
    "\n",
    "Given the input signal $x(t)$ and the left-ear impulse response, say, $h_l (t)$. Then\n",
    "$$\n",
    "\\begin{split}\n",
    "y_l[n] &= x [n] *h_l[n] \\\\\n",
    "&= \\sum_{m = -\\infty}^{\\infty} h[m]\\cdot x[n - m]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The simplest way ahead would be to compute the above summation (smartly manipulating the limits, though) using an iterative approach. The implementation is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the signals to be convolved\n",
    "source = np.random.random(5000)\n",
    "fir = np.random.random(5000)\n",
    "\n",
    "def naive_convolve(x, h):\n",
    "    # Declaring the resultant, and allocating memory\n",
    "    y = np.zeros(len(source) + len(fir) - 1)\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(h)):\n",
    "            y[i+j] += (x[i] * h[j])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.8 s, sys: 132 ms, total: 22.9 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "time naive_con = naive_convolve(source, fir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Clearly, we can see that $\\approx 23s$ for convoluting $5000\\times5000$ is very poor indeed, and can be a major bottleneck for the various applications dependent on the convolution operation. Complexity analysis for this clearly gives the time taken to be $\\mathcal{O}(m n)$, or $\\mathcal{O}(n^2)$ in general.\n",
    "\n",
    "Extending this to the required application, where we would be convoluting $200000\\times70000$ terms, extending the times and accounting for overheads gives an estimate of $\\approx 3.5hr$. The above code took $212min$ to give the convoluted result on the given input. Clearly, this is not acceptable. So how do we improve this algorithm?\n",
    "\n",
    "It is not hard to see that the computations in the outer loop are independent and hence can be run as independent threads on a parallelised processor or GPU. Given infinite cores, the execution time would be $\\mathcal{O}(n)$. Practical figures are much lower this mark. This is the motivation behind the next approach.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> Naive Implementation Parallelised </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the computations being performed tom compute the output signal at any time t don't depend in any way on the computations for any other time t'. This means that these computations can be done in parallel. Hence, given $\\textbf{n}$ enough processors, we can hence perform the convolution in $\\mathcal{O}(n)$ time. \n",
    "\n",
    "This motivates the implementation of this naive convolution algorithm on a GPU. The reason being that modern GPU have hundreds to thousands of cores, with specialised fast access cache and memory and highly optimised memory access operations and blazing fast floating point operation abilities. \n",
    "\n",
    "The following kernel has been written in CUDA-C++, a GPU programming language developed by Nvidia. CUDA is the leading standard for GPU computing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c++\n",
    "__global__ static void ConvolveCUDA(const float* a, const float* b, float* c, int n)\n",
    "{\n",
    "\tint i = 0;\n",
    "\n",
    "\tint idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "\n",
    "\tif (idx < n)\n",
    "\t{\n",
    "\t\tfloat t1 = 0; float t2 = 0;\n",
    "\t\tfor (i = 0; i <= idx; i++) {\n",
    "\t\t\tt1 += a[i] * b[idx - i];\n",
    "\t\t}\n",
    "\t\tfor (i = idx + 1; i < n; i++) {\n",
    "\t\t\tt2 += a[i] * b[n + idx - i];\n",
    "\t\t}\n",
    "\t\tc[idx] = t1;\n",
    "\t\tc[n + idx] = t2;\n",
    "\t}\n",
    "}\n",
    "\n",
    "clock_t convolveCUDA(const float *a, const float *b, float *c, int n)\n",
    "{\n",
    "\tfloat *a_d, *b_d, *c_d;\n",
    "\tclock_t start, end;\n",
    "\tint BLOCK_NUM = n / THREAD_NUM + ((n % THREAD_NUM > 0) ? 1 : 0);\n",
    "\n",
    "\tcudaMalloc((void**)&a_d, sizeof(float) * n);\n",
    "\tcudaMalloc((void**)&b_d, sizeof(float) * n);\n",
    "\tcudaMalloc((void**)&c_d, sizeof(float) * (2 * n - 1));\n",
    "\n",
    "\tstart = clock();\n",
    "\n",
    "\tcudaMemcpy(a_d, a, sizeof(float) * n, cudaMemcpyHostToDevice);\n",
    "\tcudaMemcpy(b_d, b, sizeof(float) * n, cudaMemcpyHostToDevice);\n",
    "\tcudaMemcpy(c_d, c, sizeof(float) * (2 * n - 1), cudaMemcpyHostToDevice);\n",
    "\n",
    "\tConvolveCUDA << < BLOCK_NUM, THREAD_NUM >> >(a_d, b_d, c_d, n);\n",
    "\n",
    "\tcudaMemcpy(c, c_d, sizeof(float) * (2 * n - 1), cudaMemcpyDeviceToHost);\n",
    "\n",
    "\tend = clock();\n",
    "\n",
    "\tcudaFree(a_d);\n",
    "\tcudaFree(b_d);\n",
    "\tcudaFree(c_d);\n",
    "\n",
    "\treturn end - start;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
