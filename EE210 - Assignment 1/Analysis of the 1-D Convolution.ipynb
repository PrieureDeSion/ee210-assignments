{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a [jupyter](http://jupyter.org) notebook.\n",
    "\n",
    "Open the notebook by (1) copying this file into a directory, (2) launching jupyter notebook in that directory\n",
    "and (3) selecting the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u><font color= 'blue'>Analysis of the 1-D Convolution Operation </font> </u>\n",
    "### <i>Practical Assignment 1, EE210 (Spring 2017) </i>\n",
    "___\n",
    "A notebook by ___Dhruv Ilesh Shah___ and ___Shashwat Shukla___  \n",
    "___\n",
    "__Required Packages:__ Python(2.7+), C++11, CUDA _(for the GPU implementations)_, NumPy, math, cmath, SciPy.io _(Optional, to simplify IO operations)_\n",
    "___\n",
    "In this notebook, we present an analysis of the various algorithms for computing the one-dimensional convolution operation, along with their implementation and suitable improvisations. This includes the crude implementation, parallelised implementation, Cooley-Tukey algorithm (FFT), improvised Cooley-Tukey and a vectorised FFT-based implementation. We will look at the Python and C++ implementations on both CPU and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "The convolution operation is a fundamental operation on two functions $f$ and $g$ which produces a third function, that is typically viewed as a modified version of one of the original functions, giving the integral of the pointwise multiplication of the two functions as a function of the amount that one of the original functions is translated. Convolution is similar to _cross-correlation_.\n",
    "\n",
    "$$\n",
    "(f*g)(t) \\triangleq \\int_{-\\infty}^{\\infty} f(\\tau) g(t - \\tau)\n",
    "$$\n",
    "Where $f, g : [0, \\infty) \\rightarrow \\mathbb{R}$.\n",
    "This can be extended to discrete-time signals as\n",
    "$$\n",
    "(f*g)[n] \\triangleq \\sum_{m = -\\infty}^{\\infty} f[m] g[n - m]\n",
    "$$\n",
    "\n",
    "For any physical _linear & time invariant (LTI)_ system $T$, it can be shown that the output of $T$ can be represented as $y(t) = x(t) * h(t)$, where $h(t)$ represents the response of $T$ to the unit impulse. This means that given an LTI system, the complete system can be represented solely by its impulse response $h(t)$. _(Similar definition for discrete-time signals.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- bibtex\n",
    "\n",
    "@Article{CTmain,\n",
    "Author = {James Cooley, John Tukey},\n",
    "title = {An Algorithm for Machine Calculation of Complex Fourier Series},\n",
    "journal = {American Mathematical Society},\n",
    "year = {1965}},\n",
    "\n",
    "@article{key ,\n",
    "author = {Arthur B Cummings and David Eftekhary and Frank G House},\n",
    "title = {The accurate determination of college studentsâ€™\n",
    "coefficients of friction},\n",
    "journal = {Journal of Sketchy Physics},\n",
    "volume = {13},\n",
    "year = {2003},\n",
    "number = {2},\n",
    "pages = {46--129}\n",
    "}\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "Given a single-channel audio input $(16kHz)$, and the 2-channel impulse response of the surroundings as heard by the pair of human ears, generate a stereo reconstruction of the input.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y_l (t) &= x (t) * h_l (t) \\\\\n",
    "y_r (t) &= x (t) * h_r (t)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "***\n",
    "In a typical scenario, the impulse response depends on the surroundings due to multiple reflections, interference and reverberation etc. The energy of the response, however, decays slowly and we have a finite duration response.\n",
    "\n",
    "![Impulse Response of a Hall](Files/IR_hall.jpg)\n",
    "<center>Courtesy: [ProSoundWeb](http://www.prosoundweb.com/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "As is clear from the definition, the convolution is an expensive operation and given the size of a typical sound clip $(10 sec \\rightarrow 1.6\\times10^5samples)$, the computation can be very intensive. But given the essentiality of the convolution as a fundamental property of an LTI system, and the immense number of applications, it cannot be ignored. Hence, we present an analysis of various ways to compute this convolution by employing certain other beautiful properties of this operation. We begin with algorithmic analysis for various methods to compute the convolution, and then go ahead to solve the actual problem with the best implementation. The topics covered are listed below:\n",
    "\n",
    "1. __Algorithmic Overview__\n",
    "    1. Naive Implementation\n",
    "    2. Naive Implementation Parallelised\n",
    "    3. Switching to the Frequency Domain  \n",
    "          a. Naive Fourier Transform (& Vectorization)  \n",
    "          b. Cooley-Tukey Algorithm  \n",
    "          c. Improvising the Cooley-Tukey Algorithm \n",
    "2. __Linear and Circular Convolutions__\n",
    "3. __The Final Implementation__\n",
    "4. __Final Remarks and The Road Ahead__\n",
    "\n",
    "Towards the end, we use an improvised version of the _Cooley-Tukey Algorithm_ to compute the linear convolution of the given signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6><u> Algorithmic Overview</u></font> \n",
    "\n",
    "\n",
    "# <font color=blue> Naive Implementation </font>\n",
    "\n",
    "Given the input signal $x(t)$ and the left-ear impulse response, say, $h_l (t)$. Then\n",
    "$$\n",
    "\\begin{split}\n",
    "y_l[n] &= x [n] *h_l[n] \\\\\n",
    "&= \\sum_{m = -\\infty}^{\\infty} h[m]\\cdot x[n - m]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The simplest way ahead would be to compute the above summation (smartly manipulating the limits, though) using an iterative approach. The implementation is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the signals to be convolved\n",
    "source = np.random.random(5000)\n",
    "fir = np.random.random(5000)\n",
    "\n",
    "def naive_convolve(x, h):\n",
    "    # Declaring the resultant, and allocating memory\n",
    "    y = np.zeros(len(source) + len(fir) - 1)\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(h)):\n",
    "            y[i+j] += (x[i] * h[j])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 s, sys: 108 ms, total: 23.1 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "time naive_con = naive_convolve(source, fir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Clearly, we can see that $\\approx 23s$ for convoluting $5000\\times5000$ is very poor indeed, and can be a major bottleneck for the various applications dependent on the convolution operation. Complexity analysis for this clearly gives the time taken to be $\\mathcal{O}(m n)$, or $\\mathcal{O}(n^2)$ in general.\n",
    "\n",
    "Extending this to the required application, where we would be convoluting $200000\\times70000$ terms, extending the times and accounting for overheads gives an estimate of $\\approx 3.5hr$. The above code took $212min$ to give the convoluted result on the given input. Clearly, this is not acceptable. So how do we improve this algorithm?\n",
    "\n",
    "It is not hard to see that the computations in the outer loop are independent and hence can be run as independent threads on a parallelised processor or GPU. Given infinite cores, the execution time would be $\\mathcal{O}(n)$. Practical figures are much lower this mark. This is the motivation behind the next approach.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> Naive Implementation Parallelised </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the computations being performed tom compute the output signal at any time t don't depend in any way on the computations for any other time t'. This means that these computations can be done in parallel. Hence, given $\\textbf{n}$ enough processors, we can hence perform the convolution in $\\mathcal{O}(n)$ time. \n",
    "\n",
    "This motivates the implementation of this naive convolution algorithm on a GPU. The reason being that modern GPU have hundreds to thousands of cores, with specialised fast access cache and memory and highly optimised memory access operations and blazing fast floating point operation abilities. \n",
    "\n",
    "The following kernel has been written in CUDA-C++, a GPU programming language developed by Nvidia. CUDA is the leading standard for GPU computing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c++\n",
    "__global__ static void ConvolveCUDA(const float* a, const float* b, float* c, int n)\n",
    "{\n",
    "\tint i = 0;\n",
    "\n",
    "\tint idx = threadIdx.x + blockDim.x * blockIdx.x;\n",
    "\n",
    "\tif (idx < n)\n",
    "\t{\n",
    "\t\tfloat t1 = 0; float t2 = 0;\n",
    "\t\tfor (i = 0; i <= idx; i++) {\n",
    "\t\t\tt1 += a[i] * b[idx - i];\n",
    "\t\t}\n",
    "\t\tfor (i = idx + 1; i < n; i++) {\n",
    "\t\t\tt2 += a[i] * b[n + idx - i];\n",
    "\t\t}\n",
    "\t\tc[idx] = t1;\n",
    "\t\tc[n + idx] = t2;\n",
    "\t}\n",
    "}\n",
    "\n",
    "clock_t convolveCUDA(const float *a, const float *b, float *c, int n)\n",
    "{\n",
    "\tfloat *a_d, *b_d, *c_d;\n",
    "\tclock_t start, end;\n",
    "\tint BLOCK_NUM = n / THREAD_NUM + ((n % THREAD_NUM > 0) ? 1 : 0);\n",
    "\n",
    "\tcudaMalloc((void**)&a_d, sizeof(float) * n);\n",
    "\tcudaMalloc((void**)&b_d, sizeof(float) * n);\n",
    "\tcudaMalloc((void**)&c_d, sizeof(float) * (2 * n - 1));\n",
    "\n",
    "\tstart = clock();\n",
    "\n",
    "\tcudaMemcpy(a_d, a, sizeof(float) * n, cudaMemcpyHostToDevice);\n",
    "\tcudaMemcpy(b_d, b, sizeof(float) * n, cudaMemcpyHostToDevice);\n",
    "\tcudaMemcpy(c_d, c, sizeof(float) * (2 * n - 1), cudaMemcpyHostToDevice);\n",
    "\n",
    "\tConvolveCUDA << < BLOCK_NUM, THREAD_NUM >> >(a_d, b_d, c_d, n);\n",
    "\n",
    "\tcudaMemcpy(c, c_d, sizeof(float) * (2 * n - 1), cudaMemcpyDeviceToHost);\n",
    "\n",
    "\tend = clock();\n",
    "\n",
    "\tcudaFree(a_d);\n",
    "\tcudaFree(b_d);\n",
    "\tcudaFree(c_d);\n",
    "\n",
    "\treturn end - start;\n",
    "}\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <font color=blue> Switching to the Frequency Domain </font>\n",
    "\n",
    "We saw that parallelising the algorithm to multiple cores gives a speedup of about $37.78$, but that requires a state-of-the-art GPU and a dedicated core. Looking at the enormous applications of convolution even in the simplest of games on your mobile phone, we surely need a better method to compute the convolution operation. In this section, we motivate a completely new approach that exploits the similarities between the definition of the convolution and the Fourier transform.\n",
    "\n",
    "The __convolution theorem__ states that under suitable conditions the Fourier transform of a convolution is the pointwise product of Fourier transforms. Skipping the proof, the result can be interpreted as\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathcal{F}(f*g) &= \\mathcal{F}(f) \\cdot \\mathcal{F}(g) \\\\\n",
    "f * g &= \\mathcal{F}^{-1} \\{ \\mathcal{F}(f) \\cdot \\mathcal{F}(g) \\}\n",
    "\\end{split}\n",
    "$$\n",
    "Given this bit of information, we can now approach the whole problem of convolution through this procedure.\n",
    "\n",
    "## Algorithmic Sketch\n",
    "\n",
    "Given the input signal $x(t)$ and response $h(t)$, we can now compute the convolution $y(t) = x(t) * h(t)$ as:\n",
    "\n",
    "* Convert the input signal and frequency response to the frequency domain. _(Note that this step would involve intricacies like dimension of bases etc. that we will cover during the implementation)_\n",
    "$$\n",
    "\\begin{split}\n",
    "X &= \\mathcal{F}(f(t)) \\\\\n",
    "IR &= \\mathcal{F}(h(t))\n",
    "\\end{split}\n",
    "$$\n",
    "* Compute the point-wise multiplication of the signals $X, IR$ in frequency domain.\n",
    "$$\n",
    "Y = X * IR\n",
    "$$\n",
    "* Compute the inverse fourier transform of $Y$ to obtain the convolved signal $y(t)$ in the time domain.\n",
    "$$\n",
    "y(t) = \\mathcal{F}^{-1}(Y)\n",
    "$$\n",
    "\n",
    "The point-wise multiplication is a cheap step, and hence the focus of the discussion from this point will switch to the efficiency of computing the operations $\\mathcal{F}$ and $\\mathcal{F}^{-1}$. Let us go about this computation in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For evaluating the correctness and efficiency of our algortihms, we have used NumPy as the benchmark\n",
    "import numpy.fft as benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Naive Fourier Transform</font>\n",
    "\n",
    "Just like our first approach towards computing the convolution, we begin the FFT computation by the most naive of methods - the definition. An FFT computes the DFT and produces exactly the same result as evaluating the discrete Fourier transform definition directly.\n",
    "\n",
    "Let $x_0, x_1 \\dots x_{N-1}$ be complex numbers. The DFT is defined by the formula\n",
    "$$\n",
    " X_k =  \\sum_{n=0}^{N-1} x_n e^{-i2\\pi kn/N}\n",
    "\\qquad\n",
    "k = 0,\\dots,N-1.\n",
    "$$\n",
    "\n",
    "Similar, for the inverse\n",
    "$$\n",
    " x_n =  \\frac{1}{N}\\sum_{k=0}^{N-1} X_k e^{i2\\pi kn/N}\n",
    "\\qquad\n",
    "n = 0,\\dots,N-1.\n",
    "$$\n",
    "\n",
    "The `python` implementation of this is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cmath import exp\n",
    "\n",
    "def naive_DFT(x):\n",
    "    N = len(x)\n",
    "    X = np.zeros(N, dtype=np.cfloat)\n",
    "    for k in range(N):\n",
    "        for n in range(N):\n",
    "            X[k] += x[n] * np.exp(-2j * np.pi * k * n / N)\n",
    "    return X\n",
    "\n",
    "def naive_iDFT(X):\n",
    "    N = len(X)\n",
    "    x = np.zeros(N, dtype=np.cfloat)\n",
    "    for n in range(N):\n",
    "        for k in range(N):\n",
    "            x[n] += X[k] * np.exp(2j * np.pi * k * n / N)\n",
    "    return x/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the input signal in time domain\n",
    "signal = np.random.random(256)\n",
    "\n",
    "signal_F = naive_DFT(signal)\n",
    "L = naive_iDFT(signal_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating DFT function: True\n",
      "Validating iDFT function: True\n"
     ]
    }
   ],
   "source": [
    "# Checking whether the computed values are accurate (tolerance = 1e-10)\n",
    "print \"Validating DFT function: %s\" % np.allclose(signal_F, benchmark.fft(signal), atol=1e-10)\n",
    "print \"Validating iDFT function: %s\" % np.allclose(L, benchmark.ifft(signal_F), atol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The results of the `naive_DFT` and `naive_iDFT` functions match with the results of the well-tested NumPy library and hence the code is correct. From the implementation, we can see each of the functions is $\\mathcal{O}[n^2]$ and this would blow up for the large $n$ we would be using. As a demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 9.2 s per loop\n",
      "The slowest run took 8.32 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 64.5 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "test_signal = np.random.random(1024)\n",
    "%timeit naive_DFT(test_signal)\n",
    "%timeit benchmark.fft(test_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\approx 9s$ for a signal of $10^3$ samples would extrapolate to hours of computation time. Not acceptable, especially when we see that NumPy is doing the same operation $1.25\\times 10^5x$ faster. Looking closely at the procedure, we can exploit the symmetry of the expression to reuse already computed values and speed up the computation. This is the motivation behind attempting a logarithmic-time approach for the Fourier transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Vectorizing the Naive FT\n",
    "\n",
    "Vectorization refers to the the technique of converting a series of product operations into a simpler matrix product, hence computing all the products at once. Using efficient matrix libraries, the product can be sped up by a great factor. This can be extended to our case as follows. Note that although the complexity still remains $\\mathcal{O}(n^2)$, the overheads are significantly reduced and the speedup is due to the implementation, and not algorithmic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorized_dft(x):\n",
    "    # At a unit level, we compute the vectorised ifft\n",
    "    x = np.asarray(x, dtype=np.cfloat)\n",
    "    N = x.shape[0]\n",
    "    n = np.arange(N)\n",
    "    k = n.reshape((N, 1))\n",
    "    M = np.exp(-2j * np.pi * k * n / N)\n",
    "    return np.dot(M, x)\n",
    "\n",
    "def vectorized_idft(x):\n",
    "    # At a unit level, we compute the vectorised ifft\n",
    "    x = np.asarray(x, dtype=np.cfloat)\n",
    "    N = x.shape[0]\n",
    "    n = np.arange(N)\n",
    "    k = n.reshape((N, 1))\n",
    "    M = np.exp(2j * np.pi * k * n / N)\n",
    "    return np.dot(M, x) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the input signal in time domain\n",
    "signal = np.random.random(256)\n",
    "\n",
    "signal_F = vectorized_dft(signal)\n",
    "L = vectorized_idft(signal_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating DFT function: True\n",
      "Validating iDFT function: True\n"
     ]
    }
   ],
   "source": [
    "# Checking whether the computed values are accurate (tolerance = 1e-10)\n",
    "print \"Validating DFT function: %s\" % np.allclose(signal_F, benchmark.fft(signal), atol=1e-10)\n",
    "print \"Validating iDFT function: %s\" % np.allclose(L, benchmark.ifft(signal_F), atol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us compare the time taken by this implementation with numpy and the our previous implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 20.5 ms per loop\n",
      "1 loop, best of 3: 615 ms per loop\n",
      "The slowest run took 11.54 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 21.2 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit vectorized_dft(signal)\n",
    "%timeit naive_DFT(signal)\n",
    "%timeit benchmark.fft(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive, isn't it? Simply vectorizing the code can improve the runtime by a factor of 30! This makes vectorization a very important trick while writing codes involving array multiplication etc. Vectorization becomes difficult and expensive as the size of the matrix increases, as the functions such as inverse are very intensive for very large matrices. Also, storage of matrices of the order of $10^5\\times10^5$ and larger require significant amount of memory, and generally fail to execute by normal function calls. All these restrict the application of the vectorized implementation to small-sized inputs.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <font color=blue>Cooley - Tukey Algorithm</font>\n",
    "\n",
    "In the above algorithms, we notice that for each point $X_k$, the computation is carried out independent of the other values. Also, the computation is with respect to each point in the frequency domain, independently. What if we could treat the whole series as the entity now, and reuse certain computations, instead of computing them all over again?\n",
    "\n",
    "A recursion-based algorithm was proposed in 1805 by _Carl Friedrich Gauss_, but it was not until 1965 that it was used in practice to compute the FFT. _James Cooley_ of IBM and _John Tukey_ of Princeton published an article in [AMS Journal, 1965](http://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf) reinventing the algorithm and describing how to perform it conveniently on a computer. Let us walk through the recursion.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "X_k &= \\sum_{n=0}^{N-1} x_n e^{-\\frac{2\\pi i}{N} nk} \\\\\n",
    "&= \\sum \\limits_{m=0}^{\\frac{N}{2}-1} x_{2m}e^{-\\frac{2\\pi i}{N} (2m)k}   +   \\sum \\limits_{m=0}^{\\frac{N}{2}-1} x_{2m+1} e^{-\\frac{2\\pi i}{N} (2m+1)k} \\\\\n",
    "&=\\underbrace{\\sum \\limits_{m=0}^{\\frac{N}{2}-1} x_{2m}   e^{-\\frac{2\\pi i}{N/2}\\;mk}}_{\\mathrm{DFT\\;of\\;even-indexed\\;part\\;of\\;} x_m} {} +  e^{-\\frac{2\\pi i}{N}k}\n",
    " \\underbrace{\\sum \\limits_{m=0}^{\\frac{N}{2}-1} x_{2m+1} e^{-\\frac{2\\pi i}{N/2}\\;mk}}_{\\mathrm{DFT\\;of\\;odd-indexed\\;part\\;of\\;} x_m} =  E_k + e^{-\\frac{2\\pi i}{N}k} O_k\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Here, we have split single Discrete Fourier series into two terms, which themselves look very similar to smaller DFT series - one on the odd indices, and the other on the even indices. Computationally, each term consists of $N\\times \\frac{N}{2}$ terms.  \n",
    "As of now, the computation is still $N^2$ for this series. The trick comes in making use of symmetries in each of these terms. Because the range of $k$ is $0\\leq k< N\\equiv \\frac{N}{2}$, while the range of $n$ is $0\\leq n<M\\equiv \\frac{M}{2}$, we see from the symmetry properties above that we need only perform half the computations for each sub-problem. Our $\\mathcal{O}[N^2]$ computation has become $\\mathcal{O}[M^2]$, with $M$ half the size of $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "But then, there's no reason to stop right here! As long as our smaller Fourier series have an even number of terms, we can reapply this _divide-and-conquer_ approach, halving the computational cost in each call. In the ___asymptotic limit___, this recursive approach scales as $\\mathcal{O}[NlogN]$.\n",
    "\n",
    "A suitable implementation on Python is given below.  \n",
    "_(We have avoided using NumPy here, to show that even a simple list-based implementation can give satisfactory results!)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing required packages for complex-math\n",
    "from cmath import exp, pi\n",
    "\n",
    "def fft(sequence):\n",
    "    # Cooley - Tukey Algorithm\n",
    "    seq = list(sequence)\n",
    "    N = len(seq)\n",
    "    if N <= 1:\n",
    "        return sequence\n",
    "    even = fft(seq[0::2])\n",
    "    odd =  fft(seq[1::2])\n",
    "\n",
    "    if (len(odd) < N//2 - 1):\n",
    "        print \"Error\"\n",
    "    T= [exp(-2j*pi*k/N)*odd[k] for k in range(N//2)]\n",
    "    return [even[k] + T[k] for k in range(N//2)] + \\\n",
    "            [even[k] - T[k] for k in range(N//2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the signal in time domain\n",
    "signal = np.random.random(1024)\n",
    "\n",
    "signal_F = fft(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating the FFT function: True\n"
     ]
    }
   ],
   "source": [
    "# Checking whether the computed values are accurate (tolerance = 1e-10)\n",
    "print \"Validating the FFT function: %s\" % np.allclose(signal_F, benchmark.fft(signal), atol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the performance of this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit fft(signal)\n",
    "%timeit vectorized_dft(signal)\n",
    "%timeit naive_DFT(signal)\n",
    "%timeit benchmark.fft(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Seems magical, eh?_ For a signal of $1024$ samples, the Cooley-Tukey algorithm is $\\approx 12$ times faster than the vectorized DFT implementation (which itself is $\\approx 30$ times faster than the naive DFT implementation).   \n",
    "_ __We have already achieved a speedup of 360, as compared from the naive DFT approach__, and an unimaginable amount, compared to the naive convolution_. Yet, this is $\\approx 300$ times slower than the __NumPy.fft__ implementation!  \n",
    "\n",
    "Let's take a moment to appreciate the immense amount of effort that goes into building and maintaining such packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
